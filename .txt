# import cv2
# import pytesseract
# import numpy as np

# # Set the path to Tesseract executable (adjust as per your installation)
# pytesseract.pytesseract.tesseract_cmd = r'/opt/homebrew/bin/tesseract'

# # Function to detect text in a specified region of the image
# def detect_text(image, region="full"):
#     if region == "full":
#         gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
#     elif region == "subtitle":
#         height, _ = image.shape[:2]
#         # Extract subtitle region from the bottom quarter of the frame
#         subtitle_region = image[int(3*height/4):height, :]
#         gray = cv2.cvtColor(subtitle_region, cv2.COLOR_BGR2GRAY)
#     else:
#         raise ValueError("Invalid region specified")
    
#     # Use Tesseract to extract text from the grayscale image
#     text = pytesseract.image_to_string(gray)
#     return text.strip(), len(text.strip()) > 0

# # Function to analyze a single frame and determine if it contains text
# def analyze_frame(frame):
#     # Detect text in the full frame and subtitle region
#     full_text, has_full_text = detect_text(frame, "full")
#     subtitle_text, has_subtitle = detect_text(frame, "subtitle")

#     # Determine scene type based on text presence
#     if has_full_text and not has_subtitle:
#         return "Texted", full_text
#     elif has_subtitle and not has_full_text:
#         return "Semi-Texted", subtitle_text
#     elif has_full_text and has_subtitle:
#         return "Texted", full_text
#     else:
#         return "Textless", ""

# # Function to analyze a video and collect results
# def analyze_video(video_path, interval=1):
#     # Open the video file
#     cap = cv2.VideoCapture(video_path)
#     fps = cap.get(cv2.CAP_PROP_FPS)
#     frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

#     results = []

#     # Iterate through frames at specified intervals
#     for frame_number in range(0, frame_count, int(fps * interval)):
#         cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)
#         ret, frame = cap.read()

#         if not ret:
#             break

#         timestamp = frame_number / fps
#         scene_type, detected_text = analyze_frame(frame)

#         # Store timestamp, scene type, and detected text (if any)
#         results.append((timestamp, scene_type, detected_text))

#     # Release the video capture object
#     cap.release()
#     return results

# # Function to analyze a video and print results
# def analyze_and_print_results(video_path, video_name):
#     results = analyze_video(video_path)

#     print(f"\nResults for {video_name}:")
#     for timestamp, scene_type, text in results:
#         print(f"Timestamp: {timestamp:.2f}s, Scene Type: {scene_type}")
#         if scene_type != "Textless":
#             print(f"Detected Text: {text[:100]}...")

# # Example usage: Analyze Video A
# video_a_path = "/Users/dhananjay/Downloads/test2.mp4"
# analyze_and_print_results(video_a_path, "Video A")

# # Example usage: Analyze Video B
# # video_b_path = "path/to/video_b.mp4"
# # analyze_and_print_results(video_b_path, "Video B")

# import cv2
# import pytesseract
# import numpy as np
# import streamlit as st
# from spellchecker import SpellChecker

# # Preprocessing function: grayscale conversion, adaptive thresholding, and denoising
# def preprocess_frame(frame):
#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
#     adaptive_thresh = cv2.adaptiveThreshold(
#         gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
#     denoised = cv2.medianBlur(adaptive_thresh, 3)
#     return denoised

# # Text region detection using EAST Text Detector
# def detect_text_regions(frame):
#     net = cv2.dnn.readNet("frozen_east_text_detection.pb")
    
#     blob = cv2.dnn.blobFromImage(frame, 1.0, (320, 320), (123.68, 116.78, 103.94), True, False)
#     net.setInput(blob)
#     geometry, scores = net.forward(["feature_fusion/Conv_7/Sigmoid", "feature_fusion/concat_3"])
    
#     (numRows, numCols) = scores.shape[2:4]
#     rects = []
#     confidences = []

#     for y in range(0, numRows):
#         scoresData = scores[0, 0, y]
#         xData0 = geometry[0, 0, y]
#         xData1 = geometry[0, 1, y]
#         xData2 = geometry[0, 2, y]
#         xData3 = geometry[0, 3, y]
#         anglesData = geometry[0, 4, y]
        
#         for x in range(0, numCols):
#             if scoresData[x] < 0.5:
#                 continue

#             offsetX, offsetY = (x * 4.0, y * 4.0)
#             angle = anglesData[x]
#             cos = np.cos(angle)
#             sin = np.sin(angle)
#             h = xData0[x] + xData2[x]
#             w = xData1[x] + xData3[x]
#             endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))
#             endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))
#             startX = int(endX - w)
#             startY = int(endY - h)

#             rects.append((startX, startY, endX, endY))
#             confidences.append(scoresData[x])

#     boxes = cv2.dnn.NMSBoxes(rects, confidences, 0.5, 0.4)
    
#     final_boxes = []
#     if len(boxes) > 0:
#         for i in boxes.flatten():
#             final_boxes.append(rects[i])
    
#     return final_boxes

# # Optical Character Recognition (OCR) with Tesseract
# def extract_text_from_region(frame, box):
#     startX, startY, endX, endY = box
#     text_region = frame[startY:endY, startX:endX]
#     config = "--psm 7"
#     text = pytesseract.image_to_string(text_region, config=config)
#     return text

# # Text post-processing with spell checking
# def correct_text(text):
#     spell = SpellChecker()
#     words = text.split()
#     corrected_text = " ".join([spell.correction(word) for word in words])
#     return corrected_text

# # Video analysis function to process and extract text from each frame
# def analyze_video(video_file):
#     cap = cv2.VideoCapture(video_file)
#     st_frame = st.empty()
    
#     while cap.isOpened():
#         ret, frame = cap.read()
#         if not ret:
#             break
        
#         preprocessed_frame = preprocess_frame(frame)
#         boxes = detect_text_regions(preprocessed_frame)
        
#         detected_text = ""
#         for box in boxes:
#             text = extract_text_from_region(frame, box)
#             corrected_text = correct_text(text)
#             detected_text += corrected_text + "\n"
        
#         st_frame.text(detected_text)
    
#     cap.release()

# # Streamlit UI
# st.title("Video Text Detection")
# video_file = st.file_uploader("Upload a video file", type=["mp4", "avi", "mov"])

# if video_file is not None:
#     # Convert the uploaded file to a temporary file path that OpenCV can use
#     temp_file = f"temp_{video_file.name}"
#     with open(temp_file, "wb") as f:
#         f.write(video_file.read())
#     analyze_video(temp_file)



# import streamlit as st
# import easyocr
# import cv2
# import numpy as np
# from PIL import Image
# import tempfile

# # Initialize EasyOCR reader
# reader = easyocr.Reader(['en'])

# def detect_text_easyocr(frame):
#     results = reader.readtext(frame)
#     for (bbox, text, prob) in results:
#         (top_left, top_right, bottom_right, bottom_left) = bbox
#         top_left = tuple(map(int, top_left))
#         bottom_right = tuple(map(int, bottom_right))
#         cv2.rectangle(frame, top_left, bottom_right, (0, 255, 0), 2)
#         cv2.putText(frame, text, top_left, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
#     return frame

# def resize_frame(frame, scale=5.0):
#     # Resize the frame to increase size
#     width = int(frame.shape[1] * scale)
#     height = int(frame.shape[0] * scale)
#     dimensions = (width, height)
#     resized_frame = cv2.resize(frame, dimensions, interpolation=cv2.INTER_LINEAR)
#     return resized_frame

# def process_video(video_path):
#     cap = cv2.VideoCapture(video_path)
#     frames = []
#     while cap.isOpened():
#         ret, frame = cap.read()
#         if not ret:
#             break
#         # Resize the frame
#         resized_frame = resize_frame(frame)
#         processed_frame = detect_text_easyocr(resized_frame)
#         # Convert BGR frame to RGB for Streamlit display
#         rgb_frame = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
#         frames.append(rgb_frame)
#     cap.release()
#     return frames

# st.title("Video Text Detection")

# uploaded_file = st.file_uploader("Upload a video file", type=["mp4", "avi", "mov"])

# if uploaded_file is not None:
#     with tempfile.NamedTemporaryFile(delete=False) as temp_file:
#         temp_file.write(uploaded_file.read())
#         temp_file_path = temp_file.name

#     st.video(temp_file_path)
    
#     st.write("Processing video...")
#     frames = process_video(temp_file_path)
    
#     if frames:
#         st.write("Displaying processed video:")
#         # Convert the list of frames into a video for display
#         for frame in frames:
#             st.image(frame, caption="Processed Frame", use_column_width=True)
#     else:
#         st.write("No frames to display.")

#     # Clean up temporary file
#     import os
#     os.remove(temp_file_path)

# import streamlit as st
# import easyocr
# import cv2
# import numpy as np
# from PIL import Image
# import tempfile

# # Initialize EasyOCR reader
# reader = easyocr.Reader(['en'])

# def detect_text_easyocr(frame, box_scale=0.1):
#     results = reader.readtext(frame)
#     for (bbox, text, prob) in results:
#         (top_left, top_right, bottom_right, bottom_left) = bbox
#         top_left = tuple(map(int, top_left))
#         bottom_right = tuple(map(int, bottom_right))
        
#         # Calculate the width and height of the bounding box
#         box_width = bottom_right[0] - top_left[0]
#         box_height = bottom_right[1] - top_left[1]
        
#         # Calculate margins to increase the box size
#         margin_x = int(box_width * box_scale)
#         margin_y = int(box_height * box_scale)
        
#         # Extend the bounding box by adding margins
#         new_top_left = (max(top_left[0] - margin_x, 0), max(top_left[1] - margin_y, 0))
#         new_bottom_right = (bottom_right[0] + margin_x, bottom_right[1] + margin_y)
        
#         # Draw the extended bounding box
#         cv2.rectangle(frame, new_top_left, new_bottom_right, (0, 255, 0), 2)
#         cv2.putText(frame, text, new_top_left, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
#     return frame

# def resize_frame(frame, scale=2.0):
#     # Resize the frame to increase size
#     width = int(frame.shape[1] * scale)
#     height = int(frame.shape[0] * scale)
#     dimensions = (width, height)
#     resized_frame = cv2.resize(frame, dimensions, interpolation=cv2.INTER_LINEAR)
#     return resized_frame

# def process_video(video_path):
#     cap = cv2.VideoCapture(video_path)
#     frames = []
#     while cap.isOpened():
#         ret, frame = cap.read()
#         if not ret:
#             break
#         # Resize the frame
#         resized_frame = resize_frame(frame)
#         processed_frame = detect_text_easyocr(resized_frame)
#         # Convert BGR frame to RGB for Streamlit display
#         rgb_frame = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
#         frames.append(rgb_frame)
#     cap.release()
#     return frames

# st.title("Video Text Detection")

# uploaded_file = st.file_uploader("Upload a video file", type=["mp4", "avi", "mov"])

# if uploaded_file is not None:
#     with tempfile.NamedTemporaryFile(delete=False) as temp_file:
#         temp_file.write(uploaded_file.read())
#         temp_file_path = temp_file.name

#     st.video(temp_file_path)
    
#     st.write("Processing video...")
#     frames = process_video(temp_file_path)
    
#     if frames:
#         st.write("Displaying processed video:")
#         # Convert the list of frames into a video for display
#         for frame in frames:
#             st.image(frame, caption="Processed Frame", use_column_width=True)
#     else:
#         st.write("No frames to display.")

#     # Clean up temporary file
#     import os
#     os.remove(temp_file_path)

# import streamlit as st
# import easyocr
# import cv2
# import numpy as np
# from PIL import Image
# import tempfile

# # Initialize EasyOCR reader
# reader = easyocr.Reader(['en'])

# def detect_text_easyocr(frame, box_scale=0.1):
#     results = reader.readtext(frame)
#     for (bbox, text, prob) in results:
#         (top_left, top_right, bottom_right, bottom_left) = bbox
#         top_left = tuple(map(int, top_left))
#         bottom_right = tuple(map(int, bottom_right))
        
#         # Calculate the width and height of the bounding box
#         box_width = bottom_right[0] - top_left[0]
#         box_height = bottom_right[1] - top_left[1]
        
#         # Calculate margins to increase the box size
#         margin_x = int(box_width * box_scale)
#         margin_y = int(box_height * box_scale)
        
#         # Extend the bounding box by adding margins
#         new_top_left = (max(top_left[0] - margin_x, 0), max(top_left[1] - margin_y, 0))
#         new_bottom_right = (bottom_right[0] + margin_x, bottom_right[1] + margin_y)
        
#         # Draw the extended bounding box
#         cv2.rectangle(frame, new_top_left, new_bottom_right, (0, 255, 0), 2)
#         cv2.putText(frame, text, new_top_left, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
#     return frame

# def resize_frame(frame, scale=2.0):
#     # Resize the frame to increase size
#     width = int(frame.shape[1] * scale)
#     height = int(frame.shape[0] * scale)
#     dimensions = (width, height)
#     resized_frame = cv2.resize(frame, dimensions, interpolation=cv2.INTER_LINEAR)
#     return resized_frame

# def process_video(video_path, scale=3.0):
#     cap = cv2.VideoCapture(video_path)
#     frames = []
#     while cap.isOpened():
#         ret, frame = cap.read()
#         if not ret:
#             break
#         # Resize the frame
#         resized_frame = resize_frame(frame, scale)
#         processed_frame = detect_text_easyocr(resized_frame)
#         # Convert BGR frame to RGB for Streamlit display
#         rgb_frame = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
#         frames.append(rgb_frame)
#     cap.release()
#     return frames

# st.title("Video Text Detection")

# uploaded_file = st.file_uploader("Upload a video file", type=["mp4", "avi", "mov"])

# if uploaded_file is not None:
#     with tempfile.NamedTemporaryFile(delete=False) as temp_file:
#         temp_file.write(uploaded_file.read())
#         temp_file_path = temp_file.name

#     st.video(temp_file_path)
    
#     st.write("Processing video...")
#     frames = process_video(temp_file_path, scale=2.0)  # Increase scale factor as needed
    
#     if frames:
#         st.write("Displaying processed video:")
#         # Convert the list of frames into a video for display
#         for frame in frames:
#             st.image(frame, caption="Processed Frame", use_column_width=True)
#     else:
#         st.write("No frames to display.")

#     # Clean up temporary file
#     import os
#     os.remove(temp_file_path)

# import streamlit as st
# import easyocr
# import cv2
# import numpy as np
# from PIL import Image
# import tempfile

# # Initialize EasyOCR reader
# reader = easyocr.Reader(['en'])

# def detect_text_easyocr(frame):
#     results = reader.readtext(frame)
#     for (bbox, text, prob) in results:
#         (top_left, top_right, bottom_right, bottom_left) = bbox
#         top_left = tuple(map(int, top_left))
#         bottom_right = tuple(map(int, bottom_right))
#         # Draw bounding box and text
#         cv2.rectangle(frame, top_left, bottom_right, (0, 255, 0), 2)
#         cv2.putText(frame, text, top_left, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
#     return frame

# def resize_frame(frame, scale=0.5):
#     width = int(frame.shape[1] * scale)
#     height = int(frame.shape[0] * scale)
#     dimensions = (width, height)
#     resized_frame = cv2.resize(frame, dimensions, interpolation=cv2.INTER_LINEAR)
#     return resized_frame

# def process_video(video_path, frame_skip=5, scale=0.5):
#     cap = cv2.VideoCapture(video_path)
#     frames = []
#     frame_count = 0
#     while cap.isOpened():
#         ret, frame = cap.read()
#         if not ret:
#             break
#         frame_count += 1
#         if frame_count % frame_skip == 0:
#             # Resize the frame
#             resized_frame = resize_frame(frame, scale)
#             processed_frame = detect_text_easyocr(resized_frame)
#             # Convert BGR frame to RGB for Streamlit display
#             rgb_frame = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
#             frames.append(rgb_frame)
#     cap.release()
#     return frames

# st.title("Video Text Detection")

# uploaded_file = st.file_uploader("Upload a video file", type=["mp4", "avi", "mov"])

# if uploaded_file is not None:
#     with tempfile.NamedTemporaryFile(delete=False) as temp_file:
#         temp_file.write(uploaded_file.read())
#         temp_file_path = temp_file.name

#     st.video(temp_file_path)
    
#     st.write("Processing video...")
#     frames = process_video(temp_file_path, frame_skip=5, scale=0.5)  # Adjust as needed
    
#     if frames:
#         st.write("Displaying processed video:")
#         # Convert the list of frames into a video for display
#         for frame in frames:
#             st.image(frame, caption="Processed Frame", use_column_width=True)
#     else:
#         st.write("No frames to display.")

#     # Clean up temporary file
#     import os
#     os.remove(temp_file_path)

# import streamlit as st
# import cv2
# import pytesseract
# from pytesseract import Output
# import tempfile
# import os

# # Path to Tesseract executable
# pytesseract.pytesseract.tesseract_cmd = r'/opt/homebrew/bin/tesseract'

# def extract_text_from_frame(frame):
#     gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
#     _, thresh_frame = cv2.threshold(gray_frame, 150, 255, cv2.THRESH_BINARY)
#     data = pytesseract.image_to_data(thresh_frame, output_type=Output.DICT)
#     text = ' '.join([data['text'][i] for i in range(len(data['level'])) if data['text'][i].strip()])
#     return text

# def process_video(video_path, start_time_ms, target_fps):
#     cap = cv2.VideoCapture(video_path)
#     if not cap.isOpened():
#         st.error(f"Error: Could not open video {video_path}.")
#         return

#     frame_count = 0
#     original_fps = cap.get(cv2.CAP_PROP_FPS)
#     frame_interval = int(original_fps / target_fps)

#     results = []

#     while cap.isOpened():
#         ret, frame = cap.read()
#         if not ret:
#             break

#         frame_position_ms = cap.get(cv2.CAP_PROP_POS_MSEC)
#         if frame_position_ms < start_time_ms:
#             continue

#         text = extract_text_from_frame(frame)

#         minutes, seconds = divmod(frame_position_ms // 1000, 60)
#         milliseconds = int(frame_position_ms % 1000)
#         time_str = f"{int(minutes):02d}:{int(seconds):02d}.{milliseconds:03d}"

#         results.append((frame_count, text, time_str))
#         frame_count += 1

#     cap.release()
#     return results

# def main():
#     st.title("Video Text Extraction and Classification")

#     video_file = st.file_uploader("Upload Video", type=["mp4", "mov", "avi"])
#     if video_file is not None:
#         with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
#             tmp_file.write(video_file.read())
#             temp_file_path = tmp_file.name

#         start_time_ms = st.slider("Start Time (ms)", 0, 600000, 0)
#         target_fps = st.slider("Target FPS", 1, 60, 30)

#         if st.button("Process Video"):
#             results = process_video(temp_file_path, start_time_ms, target_fps)

#             if results:
#                 st.write(f"Processed {len(results)} frames")
#                 for frame_count, text, time_str in results:
#                     st.write(f"Frame {frame_count}: Extracted Text: {text} (Time: {time_str})")
#             else:
#                 st.write("No text detected or video processing failed.")

#         os.remove(temp_file_path)

# if __name__ == "__main__":
#     main()

# import streamlit as st
# import cv2
# import pytesseract
# from pytesseract import Output
# import tempfile
# import os

# # Path to Tesseract executable
# pytesseract.pytesseract.tesseract_cmd = r'/opt/homebrew/bin/tesseract'

# def preprocess_frame(frame):
#     gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
#     blur_frame = cv2.GaussianBlur(gray_frame, (5, 5), 0)
#     _, thresh_frame = cv2.threshold(blur_frame, 150, 255, cv2.THRESH_BINARY)
#     return thresh_frame

# def extract_text_from_frame(frame):
#     preprocessing_frame = preprocess_frame(frame)
#     data = pytesseract.image_to_data(preprocessing_frame, config='--oem 3 --psm 6', output_type=Output.DICT)
#     text = ''
#     confidence_threshold = 70
#     for i in range(len(data['text'])):
#         if float(data['conf'][i]) >= confidence_threshold:
#             text += data['text'][i] + ' '
#     return text.strip()

# def process_video(video_path, start_time_ms, target_fps):
#     cap = cv2.VideoCapture(video_path)
#     if not cap.isOpened():
#         st.error(f"Error: Could not open video {video_path}.")
#         return

#     frame_count = 0
#     original_fps = cap.get(cv2.CAP_PROP_FPS)
#     frame_interval = int(original_fps / target_fps)

#     results = []

#     while cap.isOpened():
#         ret, frame = cap.read()
#         if not ret:
#             break

#         frame_position_ms = cap.get(cv2.CAP_PROP_POS_MSEC)
#         if frame_position_ms < start_time_ms:
#             continue

#         text = extract_text_from_frame(frame)

#         minutes, seconds = divmod(frame_position_ms // 1000, 60)
#         milliseconds = int(frame_position_ms % 1000)
#         time_str = f"{int(minutes):02d}:{int(seconds):02d}.{milliseconds:03d}"

#         results.append((frame_count, text, time_str))
#         frame_count += 1

#     cap.release()
#     return results

# def main():
#     st.title("Video Text Extraction and Classification")

#     video_file = st.file_uploader("Upload Video", type=["mp4", "mov", "avi"])
#     if video_file is not None:
#         with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
#             tmp_file.write(video_file.read())
#             temp_file_path = tmp_file.name

#         start_time_ms = st.slider("Start Time (ms)", 0, 600000, 0)
#         target_fps = st.slider("Target FPS", 1, 60, 30)

#         if st.button("Process Video"):
#             results = process_video(temp_file_path, start_time_ms, target_fps)

#             if results:
#                 st.write(f"Processed {len(results)} frames")
#                 for frame_count, text, time_str in results:
#                     st.write(f"Frame {frame_count}: Extracted Text: {text} (Time: {time_str})")
#             else:
#                 st.write("No text detected or video processing failed.")

#         os.remove(temp_file_path)

# if __name__ == "__main__":
#     main()


tesseract.py
# import cv2
# import pytesseract
# import numpy as np
# import streamlit as st
# from spellchecker import SpellChecker
# from transformers import BertTokenizer, BertForMaskedLM
# import torch
# import tempfile
# import os

# class VideoTextDetector:
#     def __init__(self):
#         self.spell = SpellChecker()
#         self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
#         self.bert_model = BertForMaskedLM.from_pretrained('bert-base-uncased')

#     def preprocess_image(self, image, region="full"):
#         if region == "full":
#             gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
#         elif region == "subtitle":
#             height, _ = image.shape[:2]
#             subtitle_region = image[int(3 * height / 4):height, :]
#             gray = cv2.cvtColor(subtitle_region, cv2.COLOR_BGR2GRAY)
#         else:
#             raise ValueError("Invalid region specified")

#         gray = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
#         gray = cv2.GaussianBlur(gray, (5, 5), 0)
#         return gray

#     def detect_text(self, image, region="full", confidence_threshold=0.5):
#         gray = self.preprocess_image(image, region)
#         config = f'--oem 3 --psm 6'
#         result = pytesseract.image_to_data(gray, config=config, output_type=pytesseract.Output.DICT)
#         filtered_result = [result['text'][i] for i in range(len(result['text'])) if int(result['conf'][i]) > confidence_threshold * 100]
#         return " ".join(filtered_result).strip()

#     def correct_text_with_bert(self, text):
#         tokens = self.bert_tokenizer.tokenize(text)
#         for i, token in enumerate(tokens):
#             if token not in self.spell:
#                 inputs = self.bert_tokenizer(text, return_tensors="pt")
#                 token_ids = inputs.input_ids[0]
#                 masked_position = token_ids[i + 1]
#                 token_ids[i + 1] = self.bert_tokenizer.mask_token_id
#                 with torch.no_grad():
#                     outputs = self.bert_model(input_ids=token_ids.unsqueeze(0))
#                 predicted_token_id = outputs.logits[0, i + 1].argmax()
#                 predicted_token = self.bert_tokenizer.convert_ids_to_tokens([predicted_token_id])[0]
#                 tokens[i] = predicted_token
#         return self.bert_tokenizer.convert_tokens_to_string(tokens)

#     def analyze_frame(self, frame, confidence_threshold):
#         full_text = self.detect_text(frame, "full", confidence_threshold)
#         subtitle_text = self.detect_text(frame, "subtitle", confidence_threshold)

#         if full_text and not subtitle_text:
#             return "Texted", full_text
#         elif subtitle_text and not full_text:
#             return "Semi-Textless", subtitle_text
#         elif full_text and subtitle_text:
#             return "Texted", full_text
#         else:
#             return "Textless", ""

#     def analyze_video(self, video_path, interval=1, confidence_threshold=0.5):
#         cap = cv2.VideoCapture(video_path)
#         if not cap.isOpened():
#             raise ValueError("Error opening video file")

#         fps = cap.get(cv2.CAP_PROP_FPS)
#         frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
#         results = []
#         for frame_number in range(0, frame_count, int(fps * interval)):
#             cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)
#             ret, frame = cap.read()
#             if not ret:
#                 break
            
#             scene_type, detected_text = self.analyze_frame(frame, confidence_threshold)
#             if scene_type != "Textless":
#                 corrected_text = self.correct_text_with_bert(detected_text)
#                 results.append((frame_number, scene_type, corrected_text))
            
#             # Update progress
#             progress = (frame_number + 1) / frame_count
#             st.progress(progress)

#         cap.release()
#         return results

# def main():
#     st.title("Enhanced Video Text Detection")

#     detector = VideoTextDetector()

#     uploaded_file = st.file_uploader("Upload a video file", type=["mp4", "avi", "mov", "mkv"])
    
#     if uploaded_file is not None:
#         with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp_file:
#             tmp_file.write(uploaded_file.read())
#             video_path = tmp_file.name

#         interval = st.slider("Frame interval (seconds)", 1, 10, 1)
#         confidence_threshold = st.slider("Confidence threshold", 0.0, 1.0, 0.5)

#         if st.button("Process Video"):
#             st.write("Processing...")
#             results = detector.analyze_video(video_path, interval, confidence_threshold)

#             st.write(f"Results for {uploaded_file.name}:")
#             for frame_number, scene_type, text in results:
#                 st.write(f"Frame {frame_number}: {scene_type}")
#                 if scene_type != "Textless":
#                     st.write(f"Detected Text: {text[:100]}...")

#         # Clean up the temporary file
#         os.unlink(video_path)

# if __name__ == "__main__":
#     main()

# import cv2
# import pytesseract
# import numpy as np
# import streamlit as st

# # Ensure pytesseract is set up correctly. This might vary based on your environment.
# # Example: pytesseract.pytesseract.tesseract_cmd = r'path_to_tesseract_executable'

# ## Function to detect text in an image
# def detect_text(image, region="full"):
#     """
#     This function detects text in an image based on the specified region.

#     Args:
#         image: The input image as a NumPy array.
#         region (str, optional): The region of the image to analyze. 
#             Can be "full" for the entire image or "subtitle" for the lower portion. 
#             Defaults to "full".

#     Returns:
#         tuple: A tuple containing the extracted text and a flag indicating 
#                if any text was detected.

#     Raises:
#         ValueError: If an invalid region is specified.
#     """
#     if region == "full":
#         gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
#     elif region == "subtitle":
#         height, _ = image.shape[:2]
#         subtitle_region = image[int(3*height/4):height, :]
#         gray = cv2.cvtColor(subtitle_region, cv2.COLOR_BGR2GRAY)
#     else:
#         raise ValueError("Invalid region specified")
    
#     # Detect text using pytesseract
#     text = pytesseract.image_to_string(gray)
#     return text.strip(), len(text.strip()) > 0

# ## Function to analyze a single video frame
# def analyze_frame(frame):
#     """
#     This function analyzes a single video frame to identify scene type based on text presence.

#     Args:
#         frame: The video frame as a NumPy array.

#     Returns:
#         tuple: A tuple containing the scene type ("Texted", "Semi-Textless", "Textless") 
#                and the detected text (empty string if none).
#     """
#     full_text, has_full_text = detect_text(frame, "full")
#     subtitle_text, has_subtitle = detect_text(frame, "subtitle")

#     if has_full_text and not has_subtitle:
#         return "Texted", full_text
#     elif has_subtitle and not has_full_text:
#         return "Semi-Textless", subtitle_text
#     elif has_full_text and has_subtitle:
#         return "Texted", full_text
#     else:
#         return "Textless", ""

# ## Function to analyze an entire video
# def analyze_video(video_path, interval=1):
#     """
#     This function analyzes a video and returns a list of results for each frame at a specified interval.

#     Args:
#         video_path (str): The path to the video file.
#         interval (int, optional): The interval (in seconds) between analyzed frames. Defaults to 1.

#     Returns:
#         list: A list of tuples containing (timestamp, scene type, detected text) for each analyzed frame.
#     """
#     cap = cv2.VideoCapture(video_path)
#     fps = cap.get(cv2.CAP_PROP_FPS)
#     frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

#     results = []

#     for frame_number in range(0, frame_count, int(fps * interval)):
#         cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)
#         ret, frame = cap.read()

#         if not ret:
#             break

#         timestamp = frame_number / fps
#         scene_type, detected_text = analyze_frame(frame)

#         results.append((timestamp, scene_type, detected_text))

#     cap.release()
#     return results

# # Streamlit dashboard
# st.title("Video Text Detection with pytesseract")

# # Upload video file
# uploaded_file = st.file_uploader("Upload a video file", type=["mp4", "avi", "mov", "mkv"])

# if uploaded_file is not None:
#     video_path = f"/tmp/{uploaded_file.name}"
#     with open(video_path, mode='wb') as f:
#         f.write(uploaded_file.read())

#     if st.button("Process Video"):
#         st.write("Processing...")
#         results = analyze_video(video_path)

#         st.write(f"Results for {uploaded_file.name}:")
#         for timestamp, scene_type, text in results:
#             st.write(f"Timestamp: {timestamp:.2f}s, Scene Type: {scene_type}")
#             if scene_type != "Textless":
#                 st.write(f"Detected Text: {text[:100]}...")

import cv2
import pytesseract
import numpy as np
import streamlit as st
from spellchecker import SpellChecker

# Ensure pytesseract is set up correctly. This might vary based on your environment.
# Example: pytesseract.pytesseract.tesseract_cmd = r'path_to_tesseract_executable'

spell = SpellChecker()

# Add domain-specific terms to the spell checker
custom_words = ["OpenCV", "Python", "Deep Learning", "AI", "Machine Learning"]
for word in custom_words:
    spell.word_frequency.add(word.lower())

## Function to preprocess the image
def preprocess_image(image):
    """
    This function preprocesses the input image to improve text detection accuracy.

    Args:
        image: The input image as a NumPy array.

    Returns:
        NumPy array: The preprocessed image.
    """
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    gray = cv2.medianBlur(gray, 3)
    gray = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
    gray = cv2.fastNlMeansDenoising(gray, None, 30, 7, 21)
    kernel = np.ones((1, 1), np.uint8)
    gray = cv2.dilate(gray, kernel, iterations=1)
    gray = cv2.erode(gray, kernel, iterations=1)
    return gray

## Function to postprocess the detected text
def postprocess_text(text):
    """
    This function postprocesses the detected text to improve accuracy using spell checking.

    Args:
        text: The detected text as a string.

    Returns:
        str: The postprocessed text.
    """
    words = text.split()
    corrected_words = []
    for word in words:
        corrected_word = spell.correction(word)
        if corrected_word is None:
            corrected_word = word
        corrected_words.append(corrected_word)
    return ' '.join(corrected_words)

## Function to detect text in an image
def detect_text(image, region="full"):
    """
    This function detects text in an image based on the specified region.

    Args:
        image: The input image as a NumPy array.
        region (str, optional): The region of the image to analyze. 
            Can be "full" for the entire image or "subtitle" for the lower portion. 
            Defaults to "full".

    Returns:
        tuple: A tuple containing the extracted text and a flag indicating 
               if any text was detected.

    Raises:
        ValueError: If an invalid region is specified.
    """
    if region == "full":
        gray = preprocess_image(image)
    elif region == "subtitle":
        height, _ = image.shape[:2]
        subtitle_region = image[int(3 * height / 4):height, :]
        gray = preprocess_image(subtitle_region)
    else:
        raise ValueError("Invalid region specified")

    # Detect text using pytesseract
    config = "--oem 3 --psm 6"
    text = pytesseract.image_to_string(gray, config=config)
    text = postprocess_text(text)
    return text.strip(), len(text.strip()) > 0

## Function to analyze a single video frame
def analyze_frame(frame):
    """
    This function analyzes a single video frame to identify scene type based on text presence.

    Args:
        frame: The video frame as a NumPy array.

    Returns:
        tuple: A tuple containing the scene type ("Texted", "Semi-Textless", "Textless") 
               and the detected text (empty string if none).
    """
    full_text, has_full_text = detect_text(frame, "full")
    subtitle_text, has_subtitle = detect_text(frame, "subtitle")

    if has_full_text and not has_subtitle:
        return "Texted", full_text
    elif has_subtitle and not has_full_text:
        return "Semi-Textless", subtitle_text
    elif has_full_text and has_subtitle:
        return "Texted", full_text
    else:
        return "Textless", ""

## Function to analyze an entire video
def analyze_video(video_path, interval=1):
    """
    This function analyzes a video and returns a list of results for each frame at a specified interval.

    Args:
        video_path (str): The path to the video file.
        interval (int, optional): The interval (in seconds) between analyzed frames. Defaults to 1.

    Returns:
        list: A list of tuples containing (timestamp, scene type, detected text) for each analyzed frame.
    """
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    results = []
    progress_bar = st.progress(0)

    for frame_number in range(0, frame_count, int(fps * interval)):
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)
        ret, frame = cap.read()

        if not ret:
            break

        timestamp = frame_number / fps
        scene_type, detected_text = analyze_frame(frame)

        results.append((timestamp, scene_type, detected_text))
        progress_bar.progress(frame_number / frame_count)

    cap.release()
    return results

# Streamlit dashboard
st.title("Video Text Detection with pytesseract")

# Upload video file
uploaded_file = st.file_uploader("Upload a video file", type=["mp4", "avi", "mov", "mkv"])

if uploaded_file is not None:
    video_path = f"/tmp/{uploaded_file.name}"
    with open(video_path, mode='wb') as f:
        f.write(uploaded_file.read())

    if st.button("Process Video"):
        st.write("Processing...")
        results = analyze_video(video_path)

        st.write(f"Results for {uploaded_file.name}:")
        for timestamp, scene_type, text in results:
            st.write(f"Timestamp: {timestamp:.2f}s, Scene Type: {scene_type}")
            if scene_type != "Textless":
                st.write(f"Detected Text: {text[:100]}...")



